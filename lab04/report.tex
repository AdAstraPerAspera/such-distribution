\documentclass[11pt]{article}
\usepackage{enumerate}
\usepackage{fullpage}
\usepackage{fancyhdr}
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
\pagestyle{empty}

\def\indented#1{\list{}{}\item[]}
\let\indented=\endlist

\newcounter{questionCounter}
\newcounter{partCounter}[questionCounter]
\newenvironment{question}[2][\arabic{questionCounter}]{%
    \setcounter{partCounter}{0}%
    \vspace{.25in} \hrule \vspace{0.5em}%
        \noindent{\bf #2}%
    \vspace{0.8em} \hrule \vspace{.10in}%
    \addtocounter{questionCounter}{1}%
}{}
\renewenvironment{part}[1][\alph{partCounter}]{%
    \addtocounter{partCounter}{1}%
    \vspace{.10in}%
    \begin{indented}%
       {\bf (#1)} %
}{\end{indented}}

%%%%%%%%%%%%%%%%% Identifying Information %%%%%%%%%%%%%%%%%
%% This is here, so that you can make your homework look %%
%% pretty when you compile it.                           %%
%%     DO NOT PUT YOUR NAME ANYWHERE ELSE!!!!            %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\myandrew}{William Maynes - wmaynes}
\newcommand{\partnerandrew}{Michael Wang - mhw1}
\newcommand{\myhwname}{Project 4: OpenMPI}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\thispagestyle{plain}

{\Large \myhwname} \\
\partnerandrew \\
\myandrew \\

\begin{question}{Section 1 - Parallel Design} 
\subsection*{Introduction}
When looking at how to parallel any algorithm, one of the most direct approaches is to split the work and distribute it to multiple workers. This was the fundamental idea behind the Map-Reduce framework and is the main reason why those frameworks are useful. 

When you examine the K-Means algorithm, it is simple to see that the vast majority of the work is done in the last two steps of the algorithm. In one of these steps each data value is associated with one of the means, and in the other the means are recalculated based on the points they are associated with. To parallelize the algorithm, we split up the work in each of these two steps differently.

For Data Associating:
\begin{enumerate}[]
\item Master:\\
\verb$Break data into n chunks$\\
\verb$For each node i in all nodes$\\
\verb$     Send ith chunk of data to ith node along with means$\\
\verb$     Receive results from workers$\\
\verb$Collate results$

\item Worker:\\
\verb$For each data point$\\
\verb$    Calculate distance to each mean$\\
\verb$    Group each point with closest mean$\\
\verb$Return grouped points to master$\\
\end{enumerate}
For Recalculating Means:
\begin{enumerate}[]
\item Master:\\
\verb$For each mean m$\\
\verb$    Send full data and m to worker$\\
\verb$Wait for results from workers$\\

\newpage

\item Worker:\\
\verb$Initialize totals and a counter$\\
\verb$For each piece of data d$\\
\verb$    If d is grouped with m$\\
\verb$         Add d to totals$\\
\verb$         counter++$\\
\verb$return totals / counter$
\end{enumerate}

For associating the points, since each worker is only responsible for a small chunk of the points, it is faster than the sequential version where a single processor had to associate all of the points on its own.

Also, for recalculating the means, each worker is only responsible for one of the means, as opposed to all of them in the sequential version. Thus, as the number of means and clusters increases, our algorithm actually scales better.

\end{question}


\begin{question}{Section 2 - Experimentation and Analysis}


\end{question}

\end{document}